[{"authors":null,"categories":null,"content":"Michael Luu, MPH, is a research biostatistician at the Samuel Oschin Comprehensive Cancer Institute. Luu received his master’s in public health with an emphasis in biostatistics and epidemiology at the University of Southern California in 2015. He is a proficient R programmer who was also trained in SAS, STATA and SPSS. His experience involves survival analysis, predictive modeling and data visualizations, among other areas. Current and past projects involve analysis of large national inpatient databases such as the National Cancer Database, SEER-Medicare from the National Cancer Institute, Healthcare Cost and Utilization Project, California’s Office of Statewide Health Planning and Development and Pediatrics Health Information Systems. Before joining Cedars-Sinai in 2016, he served as a biostatistician at Children’s Hospital Los Angeles for the Division of Neonatology and Department of Anesthesia Critical Care Medicine.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/michael-luu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/michael-luu/","section":"authors","summary":"Michael Luu, MPH, is a research biostatistician at the Samuel Oschin Comprehensive Cancer Institute. Luu received his master’s in public health with an emphasis in biostatistics and epidemiology at the University of Southern California in 2015.","tags":null,"title":"Michael Luu","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]]\rname = \u0026quot;Courses\u0026quot;\rurl = \u0026quot;courses/\u0026quot;\rweight = 50\r Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]]\rname = \u0026quot;Docs\u0026quot;\rurl = \u0026quot;docs/\u0026quot;\rweight = 50\r Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":["rstats","survival","ggplot2","datavis"],"content":"The Kaplan-Meier (KM) survival curves are a hallmark figure that is commonly used to illustrate \u0026ldquo;time to event\u0026rdquo; analysis in clinical research. In this illustrative example, I will be using the veterans data from the survival package to construct KM survival curves using ggplot2 and building the figure from basic geoms within the package. I will also provide examples of other publicly available packages that can facilitate in constructing KM figures that utilizes ggplot2. I believe this is a good exercise and illustrative example in potentially more advanced and little known techniques in ggplot2, as well as provide insight in the flexibility and capabilities that are available in this package.\nThe veterans data comes from a randomised trial of two treatment regimens for lung cancer. Let\u0026rsquo;s start off by loading the veterans data from the survival and have a look at the data that we are currently working with.\ndf \u0026lt;- survival::veteran %\u0026gt;% as_tibble()\rglimpse(df)\r ## Rows: 137\r## Columns: 8\r## $ trt \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...\r## $ celltype \u0026lt;fct\u0026gt; squamous, squamous, squamous, squamous, squamous, squamous...\r## $ time \u0026lt;dbl\u0026gt; 72, 411, 228, 126, 118, 10, 82, 110, 314, 100, 42, 8, 144,...\r## $ status \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1...\r## $ karno \u0026lt;dbl\u0026gt; 60, 70, 60, 60, 70, 20, 40, 80, 50, 70, 60, 40, 30, 80, 70...\r## $ diagtime \u0026lt;dbl\u0026gt; 7, 5, 3, 9, 11, 5, 10, 29, 18, 6, 4, 58, 4, 9, 11, 3, 9, 2...\r## $ age \u0026lt;dbl\u0026gt; 69, 64, 38, 63, 65, 49, 69, 68, 43, 70, 81, 63, 63, 52, 48...\r## $ prior \u0026lt;dbl\u0026gt; 0, 10, 0, 10, 10, 0, 10, 0, 0, 0, 0, 10, 0, 10, 10, 0, 0, ...\r df\r ## # A tibble: 137 x 8\r## trt celltype time status karno diagtime age prior\r## \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1 squamous 72 1 60 7 69 0\r## 2 1 squamous 411 1 70 5 64 10\r## 3 1 squamous 228 1 60 3 38 0\r## 4 1 squamous 126 1 60 9 63 10\r## 5 1 squamous 118 1 70 11 65 10\r## 6 1 squamous 10 1 20 5 49 0\r## 7 1 squamous 82 1 40 10 69 10\r## 8 1 squamous 110 1 80 29 68 0\r## 9 1 squamous 314 1 50 18 43 0\r## 10 1 squamous 100 0 70 6 70 0\r## # ... with 127 more rows\r The codebook for the dataset is provided below as follows:\n trt: 1=standard 2=test celltype: 1=squamous, 2=smallcell, 3=adeno, 4=large time: survival time (days) status: censoring status karno: Karnofsky performance score (100=good) diagtime: months from diagnosis to randomisation age: in years prior: prior therapy 0=no, 10=yes  To handle \u0026lsquo;time to event\u0026rsquo; data in R, we will first need to construct a survival object that encapsulates both the time to event information time in our dataset as well as the event/censoring variable status. We can then fit the data using the survfit() function by constructing a formula with our response variable (survival object) on the left of the ~ and the explanatory variable trt on the right. The summary() of the object from survfit() provides us the probability of survival for a given treatment over time.\nfit \u0026lt;- survfit(Surv(time, status) ~ trt, data = df)\rsummary(fit)\r ## Call: survfit(formula = Surv(time, status) ~ trt, data = df)\r## ## trt=1 ## time n.risk n.event survival std.err lower 95% CI upper 95% CI\r## 3 69 1 0.9855 0.0144 0.95771 1.000\r## 4 68 1 0.9710 0.0202 0.93223 1.000\r## 7 67 1 0.9565 0.0246 0.90959 1.000\r## 8 66 2 0.9275 0.0312 0.86834 0.991\r## 10 64 2 0.8986 0.0363 0.83006 0.973\r## 11 62 1 0.8841 0.0385 0.81165 0.963\r## 12 61 2 0.8551 0.0424 0.77592 0.942\r## 13 59 1 0.8406 0.0441 0.75849 0.932\r## 16 58 1 0.8261 0.0456 0.74132 0.921\r## 18 57 2 0.7971 0.0484 0.70764 0.898\r## 20 55 1 0.7826 0.0497 0.69109 0.886\r## 21 54 1 0.7681 0.0508 0.67472 0.874\r## 22 53 1 0.7536 0.0519 0.65851 0.862\r## 27 51 1 0.7388 0.0529 0.64208 0.850\r## 30 50 1 0.7241 0.0539 0.62580 0.838\r## 31 49 1 0.7093 0.0548 0.60967 0.825\r## 35 48 1 0.6945 0.0556 0.59368 0.812\r## 42 47 1 0.6797 0.0563 0.57782 0.800\r## 51 46 1 0.6650 0.0570 0.56209 0.787\r## 52 45 1 0.6502 0.0576 0.54649 0.774\r## 54 44 2 0.6206 0.0587 0.51565 0.747\r## 56 42 1 0.6059 0.0591 0.50040 0.734\r## 59 41 1 0.5911 0.0595 0.48526 0.720\r## 63 40 1 0.5763 0.0598 0.47023 0.706\r## 72 39 1 0.5615 0.0601 0.45530 0.693\r## 82 38 1 0.5467 0.0603 0.44049 0.679\r## 92 37 1 0.5320 0.0604 0.42577 0.665\r## 95 36 1 0.5172 0.0605 0.41116 0.651\r## 100 34 1 0.5020 0.0606 0.39615 0.636\r## 103 32 1 0.4863 0.0607 0.38070 0.621\r## 105 31 1 0.4706 0.0608 0.36537 0.606\r## 110 30 1 0.4549 0.0607 0.35018 0.591\r## 117 29 2 0.4235 0.0605 0.32017 0.560\r## 118 27 1 0.4079 0.0602 0.30537 0.545\r## 122 26 1 0.3922 0.0599 0.29069 0.529\r## 126 24 1 0.3758 0.0596 0.27542 0.513\r## 132 23 1 0.3595 0.0592 0.26031 0.496\r## 139 22 1 0.3432 0.0587 0.24535 0.480\r## 143 21 1 0.3268 0.0582 0.23057 0.463\r## 144 20 1 0.3105 0.0575 0.21595 0.446\r## 151 19 1 0.2941 0.0568 0.20151 0.429\r## 153 18 1 0.2778 0.0559 0.18725 0.412\r## 156 17 1 0.2614 0.0550 0.17317 0.395\r## 162 16 2 0.2288 0.0527 0.14563 0.359\r## 177 14 1 0.2124 0.0514 0.13218 0.341\r## 200 12 1 0.1947 0.0501 0.11761 0.322\r## 216 11 1 0.1770 0.0486 0.10340 0.303\r## 228 10 1 0.1593 0.0468 0.08956 0.283\r## 250 9 1 0.1416 0.0448 0.07614 0.263\r## 260 8 1 0.1239 0.0426 0.06318 0.243\r## 278 7 1 0.1062 0.0400 0.05076 0.222\r## 287 6 1 0.0885 0.0371 0.03896 0.201\r## 314 5 1 0.0708 0.0336 0.02793 0.180\r## 384 4 1 0.0531 0.0295 0.01788 0.158\r## 392 3 1 0.0354 0.0244 0.00917 0.137\r## 411 2 1 0.0177 0.0175 0.00256 0.123\r## 553 1 1 0.0000 NaN NA NA\r## ## trt=2 ## time n.risk n.event survival std.err lower 95% CI upper 95% CI\r## 1 68 2 0.9706 0.0205 0.93125 1.000\r## 2 66 1 0.9559 0.0249 0.90830 1.000\r## 7 65 2 0.9265 0.0317 0.86647 0.991\r## 8 63 2 0.8971 0.0369 0.82766 0.972\r## 13 61 1 0.8824 0.0391 0.80900 0.962\r## 15 60 2 0.8529 0.0429 0.77278 0.941\r## 18 58 1 0.8382 0.0447 0.75513 0.930\r## 19 57 2 0.8088 0.0477 0.72056 0.908\r## 20 55 1 0.7941 0.0490 0.70360 0.896\r## 21 54 1 0.7794 0.0503 0.68684 0.884\r## 24 53 2 0.7500 0.0525 0.65383 0.860\r## 25 51 3 0.7059 0.0553 0.60548 0.823\r## 29 48 1 0.6912 0.0560 0.58964 0.810\r## 30 47 1 0.6765 0.0567 0.57394 0.797\r## 31 46 1 0.6618 0.0574 0.55835 0.784\r## 33 45 1 0.6471 0.0580 0.54289 0.771\r## 36 44 1 0.6324 0.0585 0.52754 0.758\r## 43 43 1 0.6176 0.0589 0.51230 0.745\r## 44 42 1 0.6029 0.0593 0.49717 0.731\r## 45 41 1 0.5882 0.0597 0.48216 0.718\r## 48 40 1 0.5735 0.0600 0.46724 0.704\r## 49 39 1 0.5588 0.0602 0.45244 0.690\r## 51 38 2 0.5294 0.0605 0.42313 0.662\r## 52 36 2 0.5000 0.0606 0.39423 0.634\r## 53 34 1 0.4853 0.0606 0.37993 0.620\r## 61 33 1 0.4706 0.0605 0.36573 0.606\r## 73 32 1 0.4559 0.0604 0.35163 0.591\r## 80 31 2 0.4265 0.0600 0.32373 0.562\r## 84 28 1 0.4112 0.0597 0.30935 0.547\r## 87 27 1 0.3960 0.0594 0.29509 0.531\r## 90 25 1 0.3802 0.0591 0.28028 0.516\r## 95 24 1 0.3643 0.0587 0.26560 0.500\r## 99 23 2 0.3326 0.0578 0.23670 0.467\r## 111 20 2 0.2994 0.0566 0.20673 0.434\r## 112 18 1 0.2827 0.0558 0.19203 0.416\r## 133 17 1 0.2661 0.0550 0.17754 0.399\r## 140 16 1 0.2495 0.0540 0.16326 0.381\r## 164 15 1 0.2329 0.0529 0.14920 0.363\r## 186 14 1 0.2162 0.0517 0.13538 0.345\r## 201 13 1 0.1996 0.0503 0.12181 0.327\r## 231 12 1 0.1830 0.0488 0.10851 0.308\r## 242 10 1 0.1647 0.0472 0.09389 0.289\r## 283 9 1 0.1464 0.0454 0.07973 0.269\r## 340 8 1 0.1281 0.0432 0.06609 0.248\r## 357 7 1 0.1098 0.0407 0.05304 0.227\r## 378 6 1 0.0915 0.0378 0.04067 0.206\r## 389 5 1 0.0732 0.0344 0.02912 0.184\r## 467 4 1 0.0549 0.0303 0.01861 0.162\r## 587 3 1 0.0366 0.0251 0.00953 0.140\r## 991 2 1 0.0183 0.0180 0.00265 0.126\r## 999 1 1 0.0000 NaN NA NA\r The base R plotting method provides us with a basic KM figure. We can generate the figure by using the plot() function on the fit object.\nplot(fit)\r The GGally package also includes ggsurv() which actually uses the ggplot2 in the backend to construct the figure.\nGGally::ggsurv(fit)\r An even further improved KM figure comes from the survminer package that includes a \u0026lsquo;Number at risk\u0026rsquo; table that is commonly show in combination with the KM figure.\nsurvminer::ggsurvplot(fit, data = df, risk.table = T)\r The KM figure I\u0026rsquo;m constructing is going to be based on the survminer figure, that includes the secondary \u0026lsquo;Number at risk\u0026rsquo; table.\nWe can start by estimating the survival estimates from day 0 to day 500, I chose 500 since it appears that survival trails off after 500 days and this is a method of truncating the figure. Then we can extract the survival estimates into a a structured tidy tibble.\ns \u0026lt;- summary(fit, times = seq(0, 500, 1), extend = T)\rplot_data \u0026lt;- tibble(\r'time' = s$time,\r'n.risk' = s$n.risk,\r'n.event' = s$n.event,\r'n.censor' = s$n.censor,\r'estimate' = s$surv,\r'std.error' = s$std.err,\r'strata' = s$strata\r)\r Now that we have the \u0026lsquo;tidied\u0026rsquo; data, we can start by constructing the base plot we will use to build from. We will map the x axis to time, the y axis to estimate, and the fill to strata.\np \u0026lt;- ggplot(plot_data, aes(x = time, y = estimate, color = strata))\rp\r The primary geom in building the figure is geom_step()\np \u0026lt;- p + geom_step(aes(linetype = strata), size = 1)\rp\r This is pretty close to the plot that is provided from the GGally package already, we just need a few more steps to further clean up the axes, adjust the aesthetics, and to add a theme. I also further expanded the x axes to 550 to provide some additional room for curve annotations.\np \u0026lt;- p + scale_x_continuous(breaks = seq(0, 500, 50)) +\rscale_y_continuous(labels = scales::percent_format()) +\rtheme_classic() +\rtheme(legend.position = 'top',\raxis.title = element_text(face = 'bold')) +\rlabs(x = 'Days', y = 'Probability of Survival') + coord_cartesian(xlim = c(0, 550)) + ggsci::scale_color_d3()\rp\r We can further supplement this figure by adding markers on the curves using the ggrepel package. The simplest method I found to identify the coordinates of the ideal location of the annotations is by taking the last point of the curves by strata. Then we can use the geom_text_repel() function to add the text label to the curves accordingly. Now that we have the annotations on the figure, we can remove the legends to give the actual figure some additional room.\nannotate_data \u0026lt;- plot_data %\u0026gt;%\rgroup_by(strata) %\u0026gt;%\rslice_tail(1)\rp \u0026lt;- p + ggrepel::geom_text_repel(data = annotate_data, aes(x = time, y = estimate, label = strata),\rxlim = c(500, NA)) + theme(legend.position = 'none')\rp\r Next, we can construct the \u0026lsquo;At risk\u0026rsquo; table below the figure we just constructed. The table is actually a ggplot, where we are constructing a table of number of at risk plotted by time on the x axis and strata on the y axis. Since the time interval for the KM figure is per every 50 days, we will extract the \u0026lsquo;At risk\u0026rsquo; data similarly on a per 100 days basis. The most important concept to remember is to make the scale of the x axis scale_x_continuous() is identical to the KM figure to have the alignment match between the two. The number at risk is then plotted using geom_text() with n.risk as the label.\ntable_data \u0026lt;- plot_data %\u0026gt;% filter(\rtime %in% seq(0, 500, 50)\r) t \u0026lt;- ggplot(table_data, aes(y = fct_rev(strata), x = time)) + geom_text(aes(label = n.risk)) + scale_x_continuous(breaks = seq(0, 500, 50), limits = c(0, 550))\rt\r Now that we have a basis of the plot for the table, we can further customize it by adding a theme, and then further clean up the axes and the labels of the figure.\nt \u0026lt;- t + theme(panel.background = element_blank(),\raxis.text.x = element_blank(),\raxis.ticks = element_blank(),\raxis.title = element_blank(),\raxis.text = element_text(face = 'bold'))\rt\r We now have 2 ggplot objects, p and t. The patchwork package is the \u0026lsquo;glue\u0026rsquo; we need to put the two plots together.\nlibrary(patchwork)\rkm \u0026lt;- (p / t) + plot_layout(height = c(1, .25))\rkm\r There you have it - a publication quality KM figure ready for submission.\n","date":1602633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602633600,"objectID":"49849b8b038bd46f136f6de4db2c4486","permalink":"/post/km_ggplot/","publishdate":"2020-10-14T00:00:00Z","relpermalink":"/post/km_ggplot/","section":"post","summary":"The Kaplan-Meier (KM) survival curves are a hallmark figure that is commonly used to illustrate \"time to event\" analysis in clinical research. In this illustrative example, I will be using the `veterans` data from the `survival` package to construct KM survival curves using `ggplot2` and building the figure from basic geoms within the package.","tags":["rstats","survival","ggplot2","datavis"],"title":"Publication Quality Kaplan-Meier Survival Curves using ggplot2","type":"post"},{"authors":[],"categories":["rstats","ggplot2","datavis"],"content":"This blog post is inspired by Sharla Gefland twitter post found here, where she made a \u0026lsquo;My Life in Months\u0026rsquo; plot.\nAnnotations have always been the bane of my existence in ggplot2, and I figured this would be a fun project to get some practice. Looking at her github repo found here, she made this figure using the waffle plot package found here. Although using the waffle package may simplify some aspects of making this figure, recreating this figure in pure ggplot2 will open up the arguments for further customization that may not be available via the waffle package.\nWe can start off by creating a tibble for the basis of the plot. The goal here is to create a tibble starting from the starting month (month/year) I was born, until the current month/year. I can create this with the help of the lubridate package, which simplifies the handling of dates in R, and using this package to further extract the month and year information from the date sequence.\ndf \u0026lt;- tibble(\rdate = seq(mdy('9/1/1987'), floor_date(Sys.Date(), 'month'), 'month')\r) %\u0026gt;%\rmutate(\rmonth = month(date),\ryear = year(date)\r)\rdf \u0026lt;- tibble(date = seq(mdy('9/1/1987'), Sys.Date(), '1 month')) %\u0026gt;%\rmutate(month = month(date),\ryear = year(date))\rdf\r ## # A tibble: 398 x 3\r## date month year\r## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1987-09-01 9 1987\r## 2 1987-10-01 10 1987\r## 3 1987-11-01 11 1987\r## 4 1987-12-01 12 1987\r## 5 1988-01-01 1 1988\r## 6 1988-02-01 2 1988\r## 7 1988-03-01 3 1988\r## 8 1988-04-01 4 1988\r## 9 1988-05-01 5 1988\r## 10 1988-06-01 6 1988\r## # ... with 388 more rows\r Using the tibble I just created, I can further define the \u0026lsquo;eras\u0026rsquo; that I would like to highlight in the life plot.\nplot_data \u0026lt;- df %\u0026gt;%\rmutate(\rera = case_when(\rdate %in% mdy('9/1/1987'):mdy(\u0026quot;9/1/1991\u0026quot;) ~ 'Childhood',\rdate %in% mdy('10/1/1991'):mdy('6/1/2005') ~ 'K-12 Grade School',\rdate %in% mdy('7/1/2005'):mdy('12/1/2009') ~ 'BSc in Biological Sciences',\rdate %in% mdy('1/1/2010'):mdy('7/1/2013') ~ 'Pre Graduate Work',\rdate %in% mdy('8/1/2013'):mdy('6/1/2015') ~ 'MPH in Biostatistics \u0026amp; Epidemiology',\rdate %in% mdy('7/1/2015'):mdy('8/1/2016') ~ 'Data Analyst',\rdate %in% mdy('9/1/2016'):Sys.Date() ~ 'Biostatistician'\r)\r) %\u0026gt;%\rmutate(era = factor(\rera,\rlevels = c(\r'Childhood',\r'K-12 Grade School',\r'BSc in Biological Sciences',\r'Pre Graduate Work',\r'MPH in Biostatistics \u0026amp; Epidemiology',\r'Data Analyst',\r'Biostatistician'\r)\r))\rplot_data\r ## # A tibble: 398 x 4\r## date month year era ## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; ## 1 1987-09-01 9 1987 Childhood\r## 2 1987-10-01 10 1987 Childhood\r## 3 1987-11-01 11 1987 Childhood\r## 4 1987-12-01 12 1987 Childhood\r## 5 1988-01-01 1 1988 Childhood\r## 6 1988-02-01 2 1988 Childhood\r## 7 1988-03-01 3 1988 Childhood\r## 8 1988-04-01 4 1988 Childhood\r## 9 1988-05-01 5 1988 Childhood\r## 10 1988-06-01 6 1988 Childhood\r## # ... with 388 more rows\r Next, I\u0026rsquo;ll create a base plot using ggplot2, where I\u0026rsquo;ll map the x axis to year, and the y axis to month. I\u0026rsquo;ll also use the geom, geom_tile() to create the \u0026lsquo;blocks\u0026rsquo; that we see in the life plot, where we\u0026rsquo;ll map the fill to era.\nggplot(plot_data, aes(y = month, x = year)) + geom_tile(color = 'white', aes(fill = era), size = 1)\r Now that we have a simple base plot to work with, we can further customize and clean up the figure. A trick to give us a bigger \u0026lsquo;space\u0026rsquo; to work with is to expand the limits of the y and x axis. Furthermore, I will use scale_fill_d3() to add a fill theme to the plot.\nbase_plot \u0026lt;- ggplot(plot_data, aes(y = month, x = year)) + geom_tile(color = 'white', aes(fill = era), size = 1) + scale_y_continuous(breaks = -6:18, limits = c(-6, 18)) +\rscale_x_continuous(breaks = 1980:2020) +\rlabs(y = 'Month', x = 'Year') + theme(legend.position = 'bottom') + scale_fill_d3()\rbase_plot\r Annotations have always been tricky, because we have to specifically define the coordinates of the annotations we are trying to add. I\u0026rsquo;m going to start off small with a small annotation on the top left corner with an arrow point to the top left square. The segments are created using the geom_curve() and the text annotations are created using annotate() via geom_text()\n## annotate the definition of 1 square = 1 month\rplot \u0026lt;- base_plot +\rgeom_curve(\rx = 1987,\ry = 12,\rxend = 1986,\ryend = 14,\rcurvature = -.4,\rarrow = arrow(length = unit(0.01, \u0026quot;npc\u0026quot;), ends = 'first'),\rcolor = 'black'\r) + annotate(\r'text',\rx = 1985,\ry = 15,\rhjust = 0,\rlabel = '1 square = 1 month',\rfamily = \u0026quot;Segoe Script\u0026quot;\r)\rplot\r Next I\u0026rsquo;ll start to map out exactly where I want each of the labels for the eras to be placed. This definitely took a while, and it helps if you have some forethought on where you want to place the labels.\n### set colors pallete_colors \u0026lt;- pal_d3(\u0026quot;category10\u0026quot;)(10)\r## set size\rannotation_size \u0026lt;- 5\rplot \u0026lt;- plot + annotate(\r'text',\rx = 1989,\ry = -1,\rlabel = 'Childhood',\rcolor = pallete_colors[[1]],\rsize = annotation_size,\rfamily = \u0026quot;Segoe Script\u0026quot;\r) +\rannotate(\r'text',\rx = 1998,\ry = -1,\rlabel = 'K-12 Grade School',\rcolor = pallete_colors[[2]],\rsize = annotation_size,\rfamily = \u0026quot;Segoe Script\u0026quot;\r) +\rannotate(\r'text',\rx = 2007.5,\ry = -1,\rlabel = 'BSc in Biological Sciences',\rcolor = pallete_colors[[3]],\rsize = annotation_size,\rfamily = \u0026quot;Segoe Script\u0026quot;\r) +\rannotate(\r'text',\rx = 2011,\ry = 14,\rlabel = 'Pre Graduate Employment',\rcolor = pallete_colors[[4]],\rsize = annotation_size,\rfamily = \u0026quot;Segoe Script\u0026quot;\r) +\rannotate(\r'text',\rx = 2013,\ry = -3,\rlabel = 'MPH in Biostatistics \u0026amp; Epidemiology',\rcolor = pallete_colors[[5]],\rsize = annotation_size,\rfamily = \u0026quot;Segoe Script\u0026quot;\r) +\rannotate(\r'text',\rx = 2012.5,\ry = 16,\rlabel = 'Data Analyst',\rcolor = pallete_colors[[6]],\rsize = annotation_size,\rfamily = \u0026quot;Segoe Script\u0026quot;\r) +\rannotate(\r'text',\rx = 2018.5,\ry = -1,\rlabel = 'Biostatistician',\rcolor = pallete_colors[[7]],\rsize = annotation_size,\rfamily = \u0026quot;Segoe Script\u0026quot;\r) plot\r Now that we have the text placed in all the designated coordinates, we can start working on the arrows.\n## add additional curve segments for labels\rplot \u0026lt;- plot + geom_curve(\rx = 1989,\ry = 1,\rxend = 1989,\ryend = -.5,\rcurvature = .2,\rarrow = arrow(length = unit(0.01, 'npc'), ends = 'first'),\rcolor = 'black'\r) +\rgeom_curve(\rx = 1998,\ry = 1,\rxend = 1998,\ryend = -.5,\rcurvature = .2,\rarrow = arrow(length = unit(0.01, 'npc'), ends = 'first'),\rcolor = 'black'\r) +\rgeom_curve(\rx = 2007,\ry = 1,\rxend = 2007,\ryend = -.5,\rcurvature = -.2,\rarrow = arrow(length = unit(0.01, 'npc'), ends = 'first'),\rcolor = 'black'\r) +\rgeom_curve(\rx = 2011,\ry = 12,\rxend = 2011,\ryend = 13.5,\rcurvature = -.2,\rarrow = arrow(length = unit(0.01, 'npc'), ends = 'first'),\rcolor = 'black'\r) +\rgeom_curve(\rx = 2015,\ry = 12,\rxend = 2015,\ryend = 16,\rarrow = arrow(length = unit(0.01, 'npc'), ends = 'first'),\rcolor = 'black',\rcurvature = .8\r) +\rgeom_curve(\rx = 2014,\ry = 1,\rxend = 2014,\ryend = -2.5,\rarrow = arrow(length = unit(0.01, 'npc'), ends = 'first'),\rcurvature = -0.2,\rcolor = 'black'\r) +\rgeom_curve(\rx = 2018,\ry = 1,\rxend = 2018,\ryend = -0.5,\rarrow = arrow(length = unit(0.01, 'npc'), ends = 'first'),\rcurvature = -0.2,\rcolor = 'black'\r) plot\r Now that we have most of the annotations on there, we can add some supplemental annotations, e.g. adding an annotations regarding each column is 1 year, and the segments to finish off the look.\n## let's add a label for 1 column equals 1 year of age plot \u0026lt;- plot + annotate(\r'text',\rx = 1985,\ry = 6,\rlabel = '1 year',\rangle = 90,\rsize = 7,\rcolor = 'black',\rfamily = \u0026quot;Segoe Script\u0026quot;\r) + annotate(\r'text',\rx = 1988,\ry = 13,\rlabel = 'age',\rsize = 5,\rcolor = 'black',\rfamily = \u0026quot;Segoe Script\u0026quot;\r) +\rgeom_segment(\rx = 1988.75,\ry = 13,\rxend = 1993,\ryend = 13,\rarrow = arrow(ends = 'last', length = unit(.01, units = 'npc')),\rcolor = 'black'\r) +\rgeom_segment(\rx = 1985,\rxend = 1985,\ry = 8,\ryend = 12,\rcolor = 'black'\r) +\rgeom_segment(\rx = 1985,\rxend = 1985,\ry = 1,\ryend = 4,\rcolor = 'black'\r) +\rgeom_segment(\rx = 1984.5,\rxend = 1985.5,\ry = 12,\ryend = 12,\rcolor = 'black'\r) +\rgeom_segment(\rx = 1984.5,\rxend = 1985.5,\ry = 1,\ryend = 1,\rcolor = 'black'\r) plot\r We\u0026rsquo;re almost there - now that we have all the annotations we want on there, we can remove the legend and use a theme to further remove the grid as well as the x and y axis.\nplot \u0026lt;- plot +\rtheme_void() +\rtheme(\rlegend.position = 'none'\r)\rplot\r Let\u0026rsquo;s finish off this off by adding a title\n## lets add a title\rplot \u0026lt;- plot + annotate(\r'text',\rx = 1987,\ry = -5,\rlabel = 'Michael Luu',\rsize = 25,\rhjust = 0,\rfontface = 'bold.italic'\r)\rplot\r Full resolution figure can be found here along with the github repo for the full code here\n","date":1602028800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602028800,"objectID":"1a7d9c2a303201332412ede3b95b87b9","permalink":"/post/mylifeinplots/","publishdate":"2020-10-07T00:00:00Z","relpermalink":"/post/mylifeinplots/","section":"post","summary":"Making a life plot in R using ggplot2","tags":["rstats","ggplot2","datavis"],"title":"My life in Months - Making a 'life plot' in R using ggplot2 ","type":"post"},{"authors":[],"categories":["rstats","leaflet","datavis"],"content":"We\u0026rsquo;re going to recreate the NY Times mask-use survey data using R and the leaflet open source interactive mapping package. We can start off by loading the data from the New York Times github repository found here\nurl \u0026lt;- 'https://raw.githubusercontent.com/nytimes/covid-19-data/master/mask-use/mask-use-by-county.csv'\rdf \u0026lt;- read_csv(url)\r Now that we have the data loaded, lets have a look at the data to see what we\u0026rsquo;re working with\nglimpse(df)\r ## Rows: 3,142\r## Columns: 6\r## $ COUNTYFP \u0026lt;chr\u0026gt; \u0026quot;01001\u0026quot;, \u0026quot;01003\u0026quot;, \u0026quot;01005\u0026quot;, \u0026quot;01007\u0026quot;, \u0026quot;01009\u0026quot;, \u0026quot;01011\u0026quot;, \u0026quot;0...\r## $ NEVER \u0026lt;dbl\u0026gt; 0.053, 0.083, 0.067, 0.020, 0.053, 0.031, 0.102, 0.152, ...\r## $ RARELY \u0026lt;dbl\u0026gt; 0.074, 0.059, 0.121, 0.034, 0.114, 0.040, 0.053, 0.108, ...\r## $ SOMETIMES \u0026lt;dbl\u0026gt; 0.134, 0.098, 0.120, 0.096, 0.180, 0.144, 0.257, 0.130, ...\r## $ FREQUENTLY \u0026lt;dbl\u0026gt; 0.295, 0.323, 0.201, 0.278, 0.194, 0.286, 0.137, 0.167, ...\r## $ ALWAYS \u0026lt;dbl\u0026gt; 0.444, 0.436, 0.491, 0.572, 0.459, 0.500, 0.451, 0.442, ...\r df\r ## # A tibble: 3,142 x 6\r## COUNTYFP NEVER RARELY SOMETIMES FREQUENTLY ALWAYS\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 01001 0.053 0.074 0.134 0.295 0.444\r## 2 01003 0.083 0.059 0.098 0.323 0.436\r## 3 01005 0.067 0.121 0.12 0.201 0.491\r## 4 01007 0.02 0.034 0.096 0.278 0.572\r## 5 01009 0.053 0.114 0.18 0.194 0.459\r## 6 01011 0.031 0.04 0.144 0.286 0.5 ## 7 01013 0.102 0.053 0.257 0.137 0.451\r## 8 01015 0.152 0.108 0.13 0.167 0.442\r## 9 01017 0.117 0.037 0.15 0.136 0.56 ## 10 01019 0.135 0.027 0.161 0.158 0.52 ## # ... with 3,132 more rows\r According to the repository, the definitions of the variables are as follows:\n COUNTYFP: The county FIPS code. NEVER: The estimated share of people in this county who would say never in response to the question “How often do you wear a mask in public when you expect to be within six feet of another person?” RARELY: The estimated share of people in this county who would say rarely SOMETIMES: The estimated share of people in this county who would say sometimes FREQUENTLY: The estimated share of people in this county who would say frequently ALWAYS: The estimated share of people in this county who would say always  They are also plotting the probability of encountering a mask usage among 5 random encounters in the county.\n The chance all five people are wearing masks in five random encounters is calculated by assuming that survey respondents who answered ‘Always’ were wearing masks all of the time, those who answered ‘Frequently’ were wearing masks 80 percent of the time, those who answered ‘Sometimes’ were wearing masks 50 percent of the time, those who answered ‘Rarely’ were wearing masks 20 percent of the time and those who answered ‘Never’ were wearing masks none of the time.\n We can calculate this simply by using the supplied weights (1, .8, .5, .2, and 0) among ALWAYS, FREQUENTLY, SOMETIMES, RARELY, and NEVER mask usage, and taking the sum of the proportion of mask usage among all 5 different types of individuals that have equal probability of encountering.\ndf \u0026lt;- df %\u0026gt;%\rmutate(\rprob = (ALWAYS * 1) + (FREQUENTLY * .8) + (SOMETIMES * .5) + (RARELY * .2) + (NEVER * 0)\r)\r Since we have the county FIPS code data available, we\u0026rsquo;ll need to merge this data with county geojson data for the United States which I was able to obtain from here\ncounties \u0026lt;- rgdal::readOGR('https://eric.clst.org/assets/wiki/uploads/Stuff/gz_2010_us_050_00_500k.json')\r ## OGR data source with driver: GeoJSON ## Source: \u0026quot;https://eric.clst.org/assets/wiki/uploads/Stuff/gz_2010_us_050_00_500k.json\u0026quot;, layer: \u0026quot;gz_2010_us_050_00_500k\u0026quot;\r## with 3221 features\r## It has 6 fields\r After reading in the US counties data, we can merge the mask usage survey data with the geojson file, by the state and FIPS code. We can create a COUNTYFP variable by pasting together the STATE and COUNTY code\ncounties@data \u0026lt;- counties@data %\u0026gt;%\rmutate(\rCOUNTYFP = paste0(STATE, COUNTY)\r) %\u0026gt;%\rleft_join(\rdf\r)\r Furthermore after merging the data, we can create a label by merging together the % mask usage data into a HTML string\ncounties@data \u0026lt;- counties@data %\u0026gt;%\rmutate(\rlabel = glue::glue(\r'\u0026lt;h3\u0026gt;{NAME}\u0026lt;/h3\u0026gt;\u0026lt;br\u0026gt;\r{paste0(format(round(NEVER*100, 1), 1), \u0026quot;%\u0026quot;)} estimated NEVER wear a mask \u0026lt;br\u0026gt;\r{paste0(format(round(RARELY*100, 1), 1), \u0026quot;%\u0026quot;)} estimated RARELY wear a mask \u0026lt;br\u0026gt;\r{paste0(format(round(SOMETIMES*100, 1), 1), \u0026quot;%\u0026quot;)} estimated SOMETIMES wear a mask \u0026lt;br\u0026gt;\r{paste0(format(round(FREQUENTLY*100, 1), 1), \u0026quot;%\u0026quot;)} estimated FREQUENTLY wear a mask \u0026lt;br\u0026gt;\r{paste0(format(round(ALWAYS*100, 1), 1), \u0026quot;%\u0026quot;)} estimated ALWAYS wear a mask \u0026lt;br\u0026gt;\u0026lt;br\u0026gt;\r\u0026lt;h5\u0026gt;This translates to a \u0026lt;b\u0026gt;{paste0(format(round(prob*100, 1), 1), \u0026quot;%\u0026quot;)}\u0026lt;/b\u0026gt; chance that everyone is masked in five random encounters\u0026lt;/h5\u0026gt;'\r),\rlabel = map(label, ~ htmltools::HTML(.x))\r)\r Finally, let\u0026rsquo;s put this all together and create a Chloropleth map using the leaflet package\ncolor_pal \u0026lt;- colorNumeric('plasma', counties$prob)\rmap \u0026lt;- leaflet(counties) %\u0026gt;%\raddTiles() %\u0026gt;%\rfitBounds(lng1 = -131.519605, lng2 = -64.312607, lat1 = 50.623510, lat2 = 23.415249) %\u0026gt;%\raddPolygons(\rfillColor = ~ color_pal(prob),\rfillOpacity = .75,\rweight = 1,\rcolor = 'white',\rlabel = ~ label\r) %\u0026gt;%\raddLegend(\rposition = 'bottomright',\rpal = color_pal,\rvalues = ~ counties$prob,\rtitle = '% Mask Usage',\rlabFormat = labelFormat(\rsuffix = '%',\rtransform = function(x)\rx * 100\r)\r)\r# htmlwidgets::saveWidget(map, 'map.html', selfcontained = T)\r  \n","date":1601769600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601769600,"objectID":"0d3264f0107860b6ff18268c57f44382","permalink":"/post/covid19_mask_usage/","publishdate":"2020-10-04T00:00:00Z","relpermalink":"/post/covid19_mask_usage/","section":"post","summary":"Recreating the New York Times mask utilization survey data with the R opensource Leaflet package","tags":["rstats","leaflet","datavis"],"title":"Recreating the New York Times mask utilization survey data with the R opensource Leaflet package","type":"post"},{"authors":[],"categories":["rstats","tidymodels","nlp","textrecipes","tidytuesday"],"content":"Let\u0026rsquo;s start off by loading the data from the tidytuesday github repository.\nReading in data library(tidytuesdayR)\rlibrary(tidymodels)\rlibrary(tidyverse)\rlibrary(textrecipes)\rlibrary(future)\rlibrary(gt)\rplan(multisession)\rtheme_set(\rtheme_light() + theme(panel.grid.minor = element_blank())\r)\rbeyonce_lyrics \u0026lt;-\rreadr::read_csv(\r'https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-29/beyonce_lyrics.csv'\r) %\u0026gt;%\rjanitor::clean_names()\rtaylor_swift_lyrics \u0026lt;-\rreadr::read_csv(\r'https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-29/taylor_swift_lyrics.csv'\r) %\u0026gt;%\rjanitor::clean_names()\r Data pre-processing and feature engineering It appears the beyonce_lyrics and the taylor_swift_lyrics are the pertinent data sets for building our machine learning classifier. Let\u0026rsquo;s have a closer look at the two datasets.\nbeyonce_lyrics  ## # A tibble: 22,616 x 6\r## line song_id song_name artist_id artist_name song_line\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 If I ain't got nothing, I ~ 50396 1+1 498 Beyoncé 1\r## 2 If I ain't got something, ~ 50396 1+1 498 Beyoncé 2\r## 3 'Cause I got it with you 50396 1+1 498 Beyoncé 3\r## 4 I don't know much about al~ 50396 1+1 498 Beyoncé 4\r## 5 And it's me and you 50396 1+1 498 Beyoncé 5\r## 6 That's all we'll have when~ 50396 1+1 498 Beyoncé 6\r## 7 'Cause baby, we ain't got ~ 50396 1+1 498 Beyoncé 7\r## 8 Darling, you got enough fo~ 50396 1+1 498 Beyoncé 8\r## 9 So come on, baby, make lov~ 50396 1+1 498 Beyoncé 9\r## 10 When my days look low 50396 1+1 498 Beyoncé 10\r## # ... with 22,606 more rows\r glimpse(beyonce_lyrics)\r ## Rows: 22,616\r## Columns: 6\r## $ line \u0026lt;chr\u0026gt; \u0026quot;If I ain't got nothing, I got you\u0026quot;, \u0026quot;If I ain't got so...\r## $ song_id \u0026lt;dbl\u0026gt; 50396, 50396, 50396, 50396, 50396, 50396, 50396, 50396,...\r## $ song_name \u0026lt;chr\u0026gt; \u0026quot;1+1\u0026quot;, \u0026quot;1+1\u0026quot;, \u0026quot;1+1\u0026quot;, \u0026quot;1+1\u0026quot;, \u0026quot;1+1\u0026quot;, \u0026quot;1+1\u0026quot;, \u0026quot;1+1\u0026quot;, \u0026quot;1+1\u0026quot;,...\r## $ artist_id \u0026lt;dbl\u0026gt; 498, 498, 498, 498, 498, 498, 498, 498, 498, 498, 498, ...\r## $ artist_name \u0026lt;chr\u0026gt; \u0026quot;Beyoncé\u0026quot;, \u0026quot;Beyoncé\u0026quot;, \u0026quot;Beyoncé\u0026quot;, \u0026quot;Beyoncé\u0026quot;, \u0026quot;Beyoncé\u0026quot;, ...\r## $ song_line \u0026lt;dbl\u0026gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, ...\r taylor_swift_lyrics\r ## # A tibble: 132 x 4\r## artist album title lyrics ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Taylor S~ Taylor S~ Tim McGraw \u0026quot;He said the way my blue eyes shinx\\nPut~\r## 2 Taylor S~ Taylor S~ Picture to Burn \u0026quot;State the obvious, I didn't get my perf~\r## 3 Taylor S~ Taylor S~ Teardrops on m~ \u0026quot;Drew looks at me,\\nI fake a smile so he~\r## 4 Taylor S~ Taylor S~ A Place in Thi~ \u0026quot;I don't know what I want, so don't ask ~\r## 5 Taylor S~ Taylor S~ Cold As You \u0026quot;You have a way of coming easily to me\\n~\r## 6 Taylor S~ Taylor S~ The Outside \u0026quot;I didn't know what I would find\\nWhen I~\r## 7 Taylor S~ Taylor S~ Tied Together ~ \u0026quot;Seems the only one who doesn't see your~\r## 8 Taylor S~ Taylor S~ Stay Beautiful \u0026quot;Cory's eyes are like a jungle\\nHe smile~\r## 9 Taylor S~ Taylor S~ Should’ve Said~ \u0026quot;It's strange to think the songs we used~\r## 10 Taylor S~ Taylor S~ Mary’s Song \u0026quot;She said\\n\\\u0026quot;I was seven, and you were n~\r## # ... with 122 more rows\r glimpse(taylor_swift_lyrics)\r ## Rows: 132\r## Columns: 4\r## $ artist \u0026lt;chr\u0026gt; \u0026quot;Taylor Swift\u0026quot;, \u0026quot;Taylor Swift\u0026quot;, \u0026quot;Taylor Swift\u0026quot;, \u0026quot;Taylor Swif...\r## $ album \u0026lt;chr\u0026gt; \u0026quot;Taylor Swift\u0026quot;, \u0026quot;Taylor Swift\u0026quot;, \u0026quot;Taylor Swift\u0026quot;, \u0026quot;Taylor Swif...\r## $ title \u0026lt;chr\u0026gt; \u0026quot;Tim McGraw\u0026quot;, \u0026quot;Picture to Burn\u0026quot;, \u0026quot;Teardrops on my Guitar\u0026quot;, \u0026quot;...\r## $ lyrics \u0026lt;chr\u0026gt; \u0026quot;He said the way my blue eyes shinx\\nPut those Georgia stars...\r The beyonce_lyrics appears to be structured differently than the taylor_swift_lyrics. The lyrics from Taylor Swift is stored 1 line per title/song name, while Beyonce\u0026rsquo;s lyrics are stored by song lines. This is a problem, and we\u0026rsquo;ll have to rectify this prior to building our classifier.\nMy idea of rectifying this would be to collapse the data from Beyonce\u0026rsquo;s lyrics to get them into the same structure as Taylor Swift\u0026rsquo;s lyrics.\nbeyonce_lyrics \u0026lt;- beyonce_lyrics %\u0026gt;%\rgroup_by(\rartist_name, song_name\r) %\u0026gt;%\rsummarise(\rlyrics = paste0(line, collapse = ' ')\r) %\u0026gt;% ungroup() %\u0026gt;%\rselect(\r'artist' = artist_name,\r'title' = song_name,\rlyrics\r)\rbeyonce_lyrics\r ## # A tibble: 391 x 3\r## artist title lyrics ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Beyoncé \u0026quot;\\\u0026quot;Self-Titled\\\u0026quot; Part 1 . T~ \u0026quot;I see music. It's more than just...wha~\r## 2 Beyoncé \u0026quot;\\\u0026quot;Self-Titled\\\u0026quot; Part 2 . I~ \u0026quot;There's a moment where things click. W~\r## 3 Beyoncé \u0026quot;***Flawless (Ft. Chimamand~ \u0026quot;Your challengers are a young group fro~\r## 4 Beyoncé \u0026quot;***Flawless (Remix) (Ft. N~ \u0026quot;Dum-da-de-da Do, do, do, do, do, do (C~\r## 5 Beyoncé \u0026quot;\u0026lt;U+200B\u0026gt;come home (nala interlude)\u0026quot; \u0026quot;You have to come home We've really nee~\r## 6 Beyoncé \u0026quot;\u0026lt;U+200B\u0026gt;war (nala interlude)\u0026quot; \u0026quot;Your reign is over, Scar If you wanna ~\r## 7 Beyoncé \u0026quot;\u0026lt;U+200E\u0026gt;blind trust\u0026quot; \u0026quot;Go out tonight Feel nice tonight Stay ~\r## 8 Beyoncé \u0026quot;1+1\u0026quot; \u0026quot;If I ain't got nothing, I got you If I~\r## 9 Beyoncé \u0026quot;2017 Grammy's Best Urban C~ \u0026quot;Thank you so much. Hi, baby. Thank you~\r## 10 Beyoncé \u0026quot;6 Inch (Ft. The Weeknd)\u0026quot; \u0026quot;Six inch heels, she walked in the club~\r## # ... with 381 more rows\r Okay - this appears to be much better, and will allow us to merge them together with the Taylor Swift data. Our outcome y that we are trying to predict will be the \u0026lsquo;artist\u0026rsquo; column. The features x will be the song lyrics. In order to get them to a usable state for our model, we will have to perform some preprocessing and feature engineering.\ntaylor_swift_lyrics \u0026lt;- taylor_swift_lyrics %\u0026gt;% select(\rartist, title, lyrics\r)\rdata \u0026lt;- bind_rows(\rtaylor_swift_lyrics, beyonce_lyrics\r)\rdata\r ## # A tibble: 523 x 3\r## artist title lyrics ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Taylor Sw~ Tim McGraw \u0026quot;He said the way my blue eyes shinx\\nPut those ~\r## 2 Taylor Sw~ Picture to Burn \u0026quot;State the obvious, I didn't get my perfect fan~\r## 3 Taylor Sw~ Teardrops on my ~ \u0026quot;Drew looks at me,\\nI fake a smile so he won't ~\r## 4 Taylor Sw~ A Place in This ~ \u0026quot;I don't know what I want, so don't ask me\\n'Ca~\r## 5 Taylor Sw~ Cold As You \u0026quot;You have a way of coming easily to me\\nAnd whe~\r## 6 Taylor Sw~ The Outside \u0026quot;I didn't know what I would find\\nWhen I went l~\r## 7 Taylor Sw~ Tied Together Wi~ \u0026quot;Seems the only one who doesn't see your beauty~\r## 8 Taylor Sw~ Stay Beautiful \u0026quot;Cory's eyes are like a jungle\\nHe smiles; it's~\r## 9 Taylor Sw~ Should’ve Said No \u0026quot;It's strange to think the songs we used to sin~\r## 10 Taylor Sw~ Mary’s Song \u0026quot;She said\\n\\\u0026quot;I was seven, and you were nine\\nI ~\r## # ... with 513 more rows\r After merging the data together, we will split our data into a separate training and testing dataset.\nModel building set.seed(1)\rsplits \u0026lt;- initial_split(data, strata = artist)\rtrain \u0026lt;- training(splits)\rtest \u0026lt;- testing(splits)\r The data has now been split, where 75% of the data we have available will be used to train our classifier, and the remaining 25% will be left for validation of the model and to estimate the overall performance.\nWe will next create a \u0026lsquo;recipe\u0026rsquo; and perform feature engineering on our training data. We will do this in various steps, including tokenizing the lyrics, removing stop words, excluding words that appear less than 20 times, performing term-frequency inverse-document-frequency (TF-IDF), and finally normalization.\nrec \u0026lt;- recipe(artist ~ lyrics, data = train) %\u0026gt;%\rstep_tokenize(lyrics) %\u0026gt;%\rstep_stopwords(lyrics) %\u0026gt;%\rstep_tokenfilter(lyrics, min_times = 20, max_tokens = 500) %\u0026gt;%\rstep_tfidf(lyrics) %\u0026gt;%\rstep_normalize(all_predictors())\rrec\r ## Data Recipe\r## ## Inputs:\r## ## role #variables\r## outcome 1\r## predictor 1\r## ## Operations:\r## ## Tokenization for lyrics\r## Stop word removal for lyrics\r## Text filtering for lyrics\r## Term frequency-inverse document frequency with lyrics\r## Centering and scaling for all_predictors()\r Now that we have a \u0026lsquo;recipe\u0026rsquo; for pre-processing our data into a usable state to feed into our model, we will create a specification of the classifier. In this instance we will be building a support vector machine (SVM) classifier from the kernlab package.\nsvm_spec \u0026lt;- svm_rbf(cost = tune(), rbf_sigma = tune()) %\u0026gt;%\rset_engine('kernlab') %\u0026gt;%\rset_mode('classification')\rsvm_spec\r ## Radial Basis Function Support Vector Machine Specification (classification)\r## ## Main Arguments:\r## cost = tune()\r## rbf_sigma = tune()\r## ## Computational engine: kernlab\r Model parameter tuning The model parameters cost and rbf_sigma will be tuned via a grid search of 25 values\nsvm_wf \u0026lt;- workflow() %\u0026gt;%\radd_model(svm_spec) %\u0026gt;%\radd_recipe(rec)\rsvm_tune_folds \u0026lt;- vfold_cv(train, strata = artist)\rset.seed(1)\rsvm_tune_res \u0026lt;- tune_grid(\rsvm_wf,\rresamples = svm_tune_folds,\rgrid = 25\r)\rtune_metrics \u0026lt;- svm_tune_res %\u0026gt;% collect_metrics()\rtune_metrics %\u0026gt;%\rfilter(., .metric == 'accuracy') %\u0026gt;%\rggplot(.,\raes(y = rbf_sigma, x = cost, color = mean)) +\rgeom_point() +\rscale_color_viridis_c()\r svm_tune_res %\u0026gt;% show_best(metric = 'accuracy')\r ## # A tibble: 5 x 8\r## cost rbf_sigma .metric .estimator mean n std_err .config\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 4.92 0.00122 accuracy binary 0.822 10 0.0143 Model17\r## 2 9.16 0.000508 accuracy binary 0.812 10 0.0151 Model10\r## 3 3.54 0.000220 accuracy binary 0.789 10 0.0122 Model19\r## 4 0.0899 0.0000389 accuracy binary 0.748 10 0.00195 Model01\r## 5 0.259 0.00000000111 accuracy binary 0.748 10 0.00195 Model02\r best_accuracy \u0026lt;- svm_tune_res %\u0026gt;% select_best(., metric = 'accuracy')\rbest_accuracy\r ## # A tibble: 1 x 3\r## cost rbf_sigma .config\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 4.92 0.00122 Model17\r Final Model The optimal tuning parameters for accuracy appears to be 4.916 for cost and 0.001 for rbf_sigma. We will use these parameters for our final model. We will fit our final model on the full training data, and assess the performance on the test data.\nsvm_final_wf \u0026lt;- finalize_workflow(\rsvm_wf,\rbest_accuracy\r)\rfinal_res \u0026lt;- svm_final_wf %\u0026gt;%\rlast_fit(splits)\rfinal_metrics \u0026lt;- final_res %\u0026gt;% collect_metrics()\rfinal_metrics\r ## # A tibble: 2 x 3\r## .metric .estimator .estimate\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 accuracy binary 0.869\r## 2 roc_auc binary 0.918\r Our final model using the tuned parameters optimizing for accuracy allowed us to achieve a model accuracy of 86.9% and ROC of 0.92\nLet\u0026rsquo;s have a closer look at the performance of the model via a confusion matrix\nfinal_preds \u0026lt;- final_res %\u0026gt;%\rcollect_predictions()\rfinal_preds %\u0026gt;%\rconf_mat(\r., artist, .pred_class\r)  ## Truth\r## Prediction Beyoncé Taylor Swift\r## Beyoncé 93 13\r## Taylor Swift 4 20\r final_preds %\u0026gt;%\rconf_mat(\r., artist, .pred_class\r) %\u0026gt;%\rsummary()\r ## # A tibble: 13 x 3\r## .metric .estimator .estimate\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 accuracy binary 0.869\r## 2 kap binary 0.621\r## 3 sens binary 0.959\r## 4 spec binary 0.606\r## 5 ppv binary 0.877\r## 6 npv binary 0.833\r## 7 mcc binary 0.634\r## 8 j_index binary 0.565\r## 9 bal_accuracy binary 0.782\r## 10 detection_prevalence binary 0.815\r## 11 precision binary 0.877\r## 12 recall binary 0.959\r## 13 f_meas binary 0.916\r Closing The model appears to be doing a decent job classifying the artist by the lyrics of the songs with an overall accuracy of 86.9%. Furthermore, the model appears to be doing a better job at classifying Beyonce lyrics than Taylor Swift\u0026rsquo;s\nLet\u0026rsquo;s have a closer look at the songs that were misclassified from our model\ntest %\u0026gt;%\rselect(., -artist) %\u0026gt;%\rbind_cols(final_preds) %\u0026gt;%\rselect(\rartist, title,\r.pred_Beyoncé, `.pred_Taylor Swift`,\r.pred_class\r) %\u0026gt;%\rfilter(\rartist != .pred_class\r) %\u0026gt;%\rmutate(\racross(c(.pred_Beyoncé, `.pred_Taylor Swift`), ~ paste0(format(round(.x*100, 1), 1), '%'))\r) %\u0026gt;%\rgroup_by(., artist) %\u0026gt;%\rgt() %\u0026gt;%\rcols_label(\rartist = 'Artist',\rtitle = 'Title',\r.pred_Beyoncé = '% Beyonce',\r`.pred_Taylor Swift` = '% Taylor',\r.pred_class = 'Prediction'\r)\r html {\rfont-family: -apple-system, BlinkMacSystemFont, \u0026lsquo;Segoe UI\u0026rsquo;, Roboto, Oxygen, Ubuntu, Cantarell, \u0026lsquo;Helvetica Neue\u0026rsquo;, \u0026lsquo;Fira Sans\u0026rsquo;, \u0026lsquo;Droid Sans\u0026rsquo;, Arial, sans-serif; }\n#vvnpopnome .gt_table { display: table; border-collapse: collapse; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; }\n#vvnpopnome .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n#vvnpopnome .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; border-bottom-color: #FFFFFF; border-bottom-width: 0; }\n#vvnpopnome .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 0; padding-bottom: 4px; border-top-color: #FFFFFF; border-top-width: 0; }\n#vvnpopnome .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n#vvnpopnome .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n#vvnpopnome .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; }\n#vvnpopnome .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; }\n#vvnpopnome .gt_column_spanner_outer:first-child { padding-left: 0; }\n#vvnpopnome .gt_column_spanner_outer:last-child { padding-right: 0; }\n#vvnpopnome .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; overflow-x: hidden; display: inline-block; width: 100%; }\n#vvnpopnome .gt_group_heading { padding: 8px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; }\n#vvnpopnome .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; }\n#vvnpopnome .gt_from_md \u0026gt; :first-child { margin-top: 0; }\n#vvnpopnome .gt_from_md \u0026gt; :last-child { margin-bottom: 0; }\n#vvnpopnome .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; }\n#vvnpopnome .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 12px; }\n#vvnpopnome .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; }\n#vvnpopnome .gt_first_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; }\n#vvnpopnome .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; }\n#vvnpopnome .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; }\n#vvnpopnome .gt_striped { background-color: rgba(128, 128, 128, 0.05); }\n#vvnpopnome .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n#vvnpopnome .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; }\n#vvnpopnome .gt_footnote { margin: 0px; font-size: 90%; padding: 4px; }\n#vvnpopnome .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; }\n#vvnpopnome .gt_sourcenote { font-size: 90%; padding: 4px; }\n#vvnpopnome .gt_left { text-align: left; }\n#vvnpopnome .gt_center { text-align: center; }\n#vvnpopnome .gt_right { text-align: right; font-variant-numeric: tabular-nums; }\n#vvnpopnome .gt_font_normal { font-weight: normal; }\n#vvnpopnome .gt_font_bold { font-weight: bold; }\n#vvnpopnome .gt_font_italic { font-style: italic; }\n#vvnpopnome .gt_super { font-size: 65%; }\n#vvnpopnome .gt_footnote_marks { font-style: italic; font-size: 65%; } \n\rTitle\r% Beyonce\r% Taylor\rPrediction\r\r\rTaylor Swift\r\r\rTell Me Why\r68.3%\r31.7%\rBeyoncé\r\r\rLong Live\r82.4%\r17.6%\rBeyoncé\r\r\rRed\r69.1%\r30.9%\rBeyoncé\r\r\rHoly Ground\r58.0%\r42.0%\rBeyoncé\r\r\rSad Beautiful Tragic\r78.9%\r21.1%\rBeyoncé\r\r\rCome Back Be Here\r65.9%\r34.1%\rBeyoncé\r\r\rBlank Space\r64.9%\r35.1%\rBeyoncé\r\r\rHow You Get The Girl\r66.6%\r33.4%\rBeyoncé\r\r\rNew Romantics\r74.1%\r25.9%\rBeyoncé\r\r\rDeath By A Thousand Cuts\r80.4%\r19.6%\rBeyoncé\r\r\rAfterglow\r82.8%\r17.2%\rBeyoncé\r\r\rME\r83.5%\r16.5%\rBeyoncé\r\r\rmy tear ricochet\r76.3%\r23.7%\rBeyoncé\r\rBeyoncé\r\r\rBack to Black (Ft. André 3000)\r34.0%\r66.0%\rTaylor Swift\r\r\rNew Shoes\r6.6%\r93.4%\rTaylor Swift\r\r\rPretty Hurts\r32.2%\r67.8%\rTaylor Swift\r\r\rStop Sign\r38.5%\r61.5%\rTaylor Swift\r\r\r\r","date":1601596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601596800,"objectID":"7d9c4c2311caa2301fd46a63a2636d09","permalink":"/post/taylor_swift_and_beyonce/","publishdate":"2020-10-02T00:00:00Z","relpermalink":"/post/taylor_swift_and_beyonce/","section":"post","summary":"NLP and building a machine learning clasifier on Beyonce and Taylor Swift Lyrics #TidyTuesday","tags":["rstats","tidymodels","nlp","textrecipes","tidytuesday"],"title":"Natural Language Processing (NLP) and developing a machine learning classifier on Beyonce and Taylor Swift lyrics #TidyTuesday","type":"post"},{"authors":null,"categories":null,"content":"A web based online interactive calculator built on top of the R Shiny framework. This web application implements the enteral feeding algorithm developed internally by the quality improvement team of neonatologist and nurses at CHLA by simplifying the necessity for manual calculation, and by providing an interactive environment for custom parameters and report generation. This web application has been implemented and is in currently in clinical use at Children’s Hospital Los Angeles, LAC/USC Medical Center, and Hollywood Presbyterian Medical Center\n","date":1601337600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601337600,"objectID":"cf991a734887a27d38452fa3368f50ed","permalink":"/project/chla-nicu-feeding-calculator/","publishdate":"2020-09-29T00:00:00Z","relpermalink":"/project/chla-nicu-feeding-calculator/","section":"project","summary":"A web based online interactive calculator built on top of the R Shiny framework.","tags":["R","Shiny","Clinical"],"title":"CHLA/USC Division of Neonatology Feeding Calculator","type":"project"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot;\rif porridge == \u0026quot;blueberry\u0026quot;:\rprint(\u0026quot;Eating...\u0026quot;)\r  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}}\r{{% fragment %}} **Two** {{% /fragment %}}\r{{% fragment %}} Three {{% /fragment %}}\r Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}}\r- Only the speaker can read these notes\r- Press `S` key to view\r{{% /speaker_note %}}\r Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}}\r{{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}}\r{{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}\r  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1,\r.reveal section h2,\r.reveal section h3 {\rcolor: navy;\r}\r  Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"/contact/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"Resume","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"3960dd3bdc6f629fb800d1d2aaa7224f","permalink":"/resume/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/resume/","section":"","summary":"","tags":null,"title":"Resume","type":"widget_page"}]